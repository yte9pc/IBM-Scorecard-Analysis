---
title: "Employee Attrition"
output: html_notebook
urlcolor: blue
author: Jon Gomez (jag2j), Michael Langmayr, Nathan England, and Yihnew Eshetu
---


# About the data

We will analyze the "IBM HR Analytics Employee Attrition & Performance" dataset ((link)[https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset]).  The dataset documentation says that it was synthesized by data scientists at IBM.  Observations describe hypothetical employees.  Rows measure the employee retention status, metrics about the workplace, and details about the employee.

# Preparation

## Libraries

```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(leaps))     # model selection tools
suppressPackageStartupMessages(library(faraway))   # VIF function

source('./lib/fmrhs.R') # requires tidyverse
source('./lib/Pretty correlation.R')
```

## Loading the data
We load the data using a custom column mapping.  A few considerations apply:

* Several columns are are originally numeric levels.  We recode these from the data description.
* A few columns are irrelevant or constant across the data.  We drop these.

```{r}
# column mappings
types = cols(
  # continuous variables
  Age = col_double(),              DailyRate = col_double(),  
  DistanceFromHome = col_double(), HourlyRate = col_double(),       
  JobLevel = col_double(),
  
  # ordered factors we later recode
  #   e.g., JobSatisfaction ranges from 1 - 4 (= 'Low' to 'Very High')
  Education                = col_factor(levels = 1:5, ordered = F),
  EnvironmentSatisfaction  = col_factor(1:4, ordered = F),
  JobInvolvement           = col_factor(1:4, ordered = F),
  JobSatisfaction          = col_factor(levels = 1:4, ordered = F),
  PerformanceRating        = col_factor(levels = 1:4, ordered = F),
  RelationshipSatisfaction = col_factor(levels = 1:4, ordered = F),
  WorkLifeBalance          = col_factor(levels = 1:4, ordered = F),
  
  # other factors
  BusinessTravel =  col_factor(levels = c('Non-Travel', 'Travel_Rarely', 'Travel_Frequently'), 
                               ordered = F),
  Gender = col_factor(),            JobRole = col_factor(),
  MaritalStatus = col_factor(),     EducationField = col_factor(),
  
  # things we have decided to treat as factors
  Department = col_factor(),        # we presume we could map to a taxonomy
  StockOptionLevel = col_factor(),  # unclear scale
  
  # true/false
  Attrition = col_factor(),
  OverTime = col_factor(),
  
  # drop
  EmployeeCount = col_skip(),  # always 1
  StandardHours = col_skip(),  # always 40
  EmployeeNumber = col_skip(), # 1, 2, ...
  Over18 = col_skip(),         # always true
  
  # continous values
  MonthlyIncome = col_double(),       MonthlyRate = col_double(),              
  NumCompaniesWorked = col_double(),  PercentSalaryHike = col_double(),
  TotalWorkingYears = col_double(),   TrainingTimesLastYear = col_double(),
  WorkLifeBalance = col_double(),     YearsAtCompany = col_double(),           
  YearsInCurrentRole = col_double(),  YearsSinceLastPromotion = col_double(),
  YearsWithCurrManager = col_double()
)

# read in using mapping
ibm = read_csv("data/ibm.csv", col_types = types)

# rename levels
ibm$Education = recode(ibm$Education, 
                       '1' = 'Below College', '2' = 'College', '3' = 'Bachelor', '4' = 'Master', '5' = 'Doctor')
ibm$EnvironmentSatisfaction = recode(ibm$EnvironmentSatisfaction, 
                       '1' = 'Low', '2' = 'Medium', '3' = 'High', '4' = 'Very High')
ibm$JobInvolvement = recode(ibm$JobInvolvement, 
                       '1' = 'Low', '2' = 'Medium', '3' = 'High', '4' = 'Very High')
ibm$JobSatisfaction = recode(ibm$JobSatisfaction, 
                       '1' = 'Low', '2' = 'Medium', '3' = 'High', '4' = 'Very High')
ibm$PerformanceRating = recode(ibm$PerformanceRating, 
                       '1' = 'Low', '2' = 'Good', '3' = 'Excellent', '4' = 'Outstanding')
ibm$RelationshipSatisfaction = recode(ibm$RelationshipSatisfaction, 
                       '1' = 'Low', '2' = 'Medium', '3' = 'High', '4' = 'Very High')
ibm$WorkLifeBalance = recode(ibm$WorkLifeBalance, 
                       '1' = 'Bad', '2' = 'Good', '3' = 'Better', '4' = 'Best')

# set contrasts for yes/no levels
ibm$Attrition = relevel(ibm$Attrition, ref = "No")
ibm$OverTime = relevel(ibm$OverTime, ref = "No")

```

## Additional data manipulations

We produce a defactored version of the data for which we have replaced factors with numeric vectors.
```{r}
ibm.defactored = mutate_if(ibm, is.factor, ~ as.numeric(.x))
```

## Sample observations

We show some sample data:
```{r}
# a sample observation
t(head(ibm, n = 1))
```

```{r}
head(ibm, n = 10)[,c(1:5)]
```

# General Observations

There are 1,470 rows and 31 columns.  

```{r}

```




```{r}
par(mfrow=c(2,2))
plot(ibm$EducationField)
plot(ibm$JobInvolvement)
plot(ibm$Education)
with(ibm, hist(Age))
```

```{r}

```

```{r}
summary(ibm)
```

## Correlation

We look at columsn for which the correlation is greater than 70 in the defactored data:
```{r}
p = round(cor(ibm.defactored) * 100)
for(i in 1:nrow(p)) {
  for(j in i:ncol(p)) {
    if(i == j) next
    val = p[i,j]
    if(abs(val) > 70) {
      r = row.names(p)[i]
      c = colnames(p)[j]
      str = paste("cor(", r, ", ", c, ") = ", val, "\n")
      cat(str)
    }
  }
}
```


We also examine the VIFs of the defactored data for general interest.  We perform a formal test when we investigate specific models.
```{r}
model.defactored = lm(MonthlyIncome ~ ., data = ibm.defactored)
model.defactored.vifs = faraway::vif(model.defactored)

# Five highest VIFs with this "defactored" model on Monthly income
# 
model.defactored.vifs[order(model.defactored.vifs, decreasing = T)][1:5]
```



# Question 1

Our first question is

> What factors correlate with attrition?

## Work

### Selecting an initial model

We choose to investigate this using the leaps framework to evaluate possible variables to include.  Given the fairly small data set, we perform an exhaustive search using `regsubsets` from the leaps package.

```{r}
# this took about six minutes on one computer tested
system.time({
  allreg = regsubsets(Attrition ~ ., ibm, nbest = 1, really.big = T)
})
```

We now print out the top results for several criteria.
```{r}
pp_allreg(allreg)
```



```{r}

##intercept only model
regnull <- lm(y~1, data=data)
##model with all predictors
regfull <- lm(y~., data=data)

##forward selection, backward elimination, and stepwise regression
step(regnull, scope=list(lower=regnull, upper=regfull), direction="forward")
step(regfull, scope=list(lower=regnull, upper=regfull), direction="backward")
step(regnull, scope=list(lower=regnull, upper=regfull), direction="both")
```

### Initial model evaluation

Since the model with the extreme value for every criterion is the same in the exhautive search, we fit the specified model:
```{r}
model.allreg = glm(Attrition ~ 1 + Age + BusinessTravel + JobRole + JobSatisfaction + NumCompaniesWorked + OverTime + StockOptionLevel, data=ibm, family = "binomial")

library(glmulti)
glmulti.logistic.out <-
    glmulti(Attrition ~. , data = ibm,
            level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # glm function
            family = binomial)       # binomial family for logistic regression
## Show 5 best models (Use @ instead of $ for an S4 object)
glmulti.logistic.out@formulas
```

```{r}
groupedIbm = ibm %>% group_by(NumCompaniesWorked) %>% summarise(yeses = sum(Attrition == "Yes"), n = n())
groupedIbm$yeses/groupedIbm$n
prop <- groupedIbm$yeses/groupedIbm$n
# prop

length(unique(ibm$Age))

plot(groupedIbm$NumCompaniesWorked, log(prop/(1-prop)), ylim=c(-5,5), xlab= "Age", ylab="sample log odds")

# try glm.best
```

***CORRECT ME.***

The equation would be 
$$\begin{array}{rcl}
\text{Monthly_Income} &=& \beta_0 + \beta_1 \times \text{BusinessTravel} + \beta_2 \times \text{EnvironmentSatisfaction} 
                    \\ && + \beta_3 \times \text{JobInvolvement} + \beta_4 \times \text{JobRole} + \beta_5 \times \text{JobSatisfaction}
                    \\ && + \beta_6 \times \text{OverTime} + \beta_7 \times \text{StockOptionLevel} + \beta_8 \times \text{TotalWorkingYears}
\end{array}$$

#### Basic features

```{r}
summary(model.allreg)
```

```{r}
model.joblevel = lm(MonthlyIncome ~ JobLevel, data = ibm)
summary(model.joblevel)
```


#### Multicollinearity

We first check for multicollinearity:

```{r}
faraway::vif(model.allreg)
```


#### Linear assumptions

#### Model Diagnostics

We now check for predictive performance.




```{r}
cor(ibm.defactored)

```


