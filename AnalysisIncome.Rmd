---
title: "Employee Attrition"
output: html_notebook
urlcolor: blue
author: Jon Gomez (jag2j), Michael Langmayr, Nathan England, and Yihnew Eshetu
---


# About the data

We will analyze the "IBM HR Analytics Employee Attrition & Performance" dataset ((link)[https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset]).  The dataset documentation says that it was synthesized by data scientists at IBM.  Observations describe hypothetical employees.  Rows measure the employee retention status, metrics about the workplace, and details about the employee.

# Preparation

## Libraries

```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(leaps))     # model selection tools
suppressPackageStartupMessages(library(faraway))   # VIF function
suppressPackageStartupMessages(library(MASS))       # Box-Cox
source('./lib/fmrhs.R') # requires tidyverse
source('./lib/Pretty correlation.R')
```

## Loading the data
We load the data using a custom column mapping.  A few considerations apply:

* Several columns are originally numeric levels.  We recode these from the data description.
* A few columns are irrelevant or constant across the data.  We drop these.

```{r}
# ordered factor
ordered_factor = col_factor(levels = 1:5, ordered = F)

# column mappings
types = cols(
  # continuous variables
  Age = col_double(),              DailyRate = col_double(),  
  DistanceFromHome = col_double(), HourlyRate = col_double(),       
  #JobLevel = col_double(),
  JobLevel = col_factor(),
  # ordered factors we later recode
  #   e.g., JobSatisfaction ranges from 1 - 4 (= 'Low' to 'Very High')
  Education                = col_factor(levels = 1:5, ordered = F),
  EnvironmentSatisfaction  = col_factor(1:4, ordered = F),
  JobInvolvement           = col_factor(1:4, ordered = F),
  JobSatisfaction          = col_factor(levels = 1:4, ordered = F),
  PerformanceRating        = col_factor(levels = 1:4, ordered = F),
  RelationshipSatisfaction = col_factor(levels = 1:4, ordered = F),
  WorkLifeBalance          = col_factor(levels = 1:4, ordered = F),
  
  # other factors
  BusinessTravel =  col_factor(levels = c('Non-Travel', 'Travel_Rarely', 'Travel_Frequently'), 
                               ordered = F),
  Gender = col_factor(),            JobRole = col_factor(),
  MaritalStatus = col_factor(),     EducationField = col_factor(),
  
  # things we have decided to treat as factors
  Department = col_factor(),        # we presume we could map to a taxonomy
  StockOptionLevel = col_factor(),  # unclear scale
  
  # true/false
  Attrition = col_factor(),
  OverTime = col_factor(),
  
  # drop
  EmployeeCount = col_skip(),  # always 1
  StandardHours = col_skip(),  # always 80
  EmployeeNumber = col_skip(), # 1, 2, ...
  Over18 = col_skip(),         # always true
  
  # continous values
  MonthlyIncome = col_double(),       MonthlyRate = col_double(),              
  NumCompaniesWorked = col_double(),  PercentSalaryHike = col_double(),
  TotalWorkingYears = col_double(),   TrainingTimesLastYear = col_double(),
  WorkLifeBalance = col_double(),     YearsAtCompany = col_double(),           
  YearsInCurrentRole = col_double(),  YearsSinceLastPromotion = col_double(),
  YearsWithCurrManager = col_double()
)

# read in using mapping
ibm = read_csv("data/ibm.csv", col_types = types)

# rename levels
ibm$Education = recode(ibm$Education, 
                       '1' = 'Below College', '2' = 'College', '3' = 'Bachelor', '4' = 'Master', '5' = 'Doctor')
ibm$EnvironmentSatisfaction = recode(ibm$EnvironmentSatisfaction, 
                       '1' = 'Low', '2' = 'Medium', '3' = 'High', '4' = 'Very High')
ibm$JobInvolvement = recode(ibm$JobInvolvement, 
                       '1' = 'Low', '2' = 'Medium', '3' = 'High', '4' = 'Very High')
ibm$JobSatisfaction = recode(ibm$JobSatisfaction, 
                       '1' = 'Low', '2' = 'Medium', '3' = 'High', '4' = 'Very High')
ibm$PerformanceRating = recode(ibm$PerformanceRating, 
                       '1' = 'Low', '2' = 'Good', '3' = 'Excellent', '4' = 'Outstanding')
ibm$RelationshipSatisfaction = recode(ibm$RelationshipSatisfaction, 
                       '1' = 'Low', '2' = 'Medium', '3' = 'High', '4' = 'Very High')
ibm$WorkLifeBalance = recode(ibm$WorkLifeBalance, 
                       '1' = 'Bad', '2' = 'Good', '3' = 'Better', '4' = 'Best')

# set contrasts for yes/no levels
ibm$Attrition = relevel(ibm$Attrition, ref = "No")
ibm$OverTime = relevel(ibm$OverTime, ref = "No")

```

## Additional data manipulations

We produce a defactored version of the data for which we have replaced factors with numeric vectors.
```{r}
ibm.defactored = mutate_if(ibm, is.factor, ~ as.numeric(.x))
```

## Sample observations

We show some sample data:
```{r}
# a sample observation
t(head(ibm, n = 1))
```

```{r}
head(ibm, n = 10)[,c(1:5)]
```

# General Observations

There are 1,470 rows and 31 columns.  

```{r}

```




```{r}
par(mfrow=c(2,2))
plot(ibm$EducationField)
plot(ibm$JobInvolvement)
plot(ibm$Education)
with(ibm, hist(Age))
```

```{r}

```

```{r}
summary(ibm)
```

## Correlation

We look at columns for which the correlation is greater than 70 in the defactored data:
```{r}
p = round(cor(ibm.defactored) * 100)
for(i in 1:nrow(p)) {
  for(j in i:ncol(p)) {
    if(i == j) next
    val = p[i,j]
    if(abs(val) > 70) {
      r = row.names(p)[i]
      c = colnames(p)[j]
      str = paste("cor(", r, ", ", c, ") = ", val, "\n")
      cat(str)
    }
  }
}
```


We also examine the VIFs of the defactored data for general interest.  We perform a formal test when we investigate specific models.
```{r}
model.defactored = lm(MonthlyIncome ~ ., data = ibm.defactored)
model.defactored.vifs = faraway::vif(model.defactored)

# Five highest VIFs with this "defactored" model on Monthly income
# 
model.defactored.vifs[order(model.defactored.vifs, decreasing = T)][1:5]
```



# Question 1

Our first question is

> What factors correlate with attrition?

## Work

### Selecting an initial model

We choose to investigate this using the leaps framework to evaluate possible variables to include.  Given the fairly small data set, we perform an exhaustive search using `regsubsets` from the leaps package.

```{r}
# this took about six minutes on one computer tested
system.time({
  allreg = regsubsets(MonthlyIncome ~ ., ibm, nbest = 1, really.big = T)
})
```

We now print out the top results for several criteria.
```{r}
pp_allreg(allreg)
```

### Initial model evaluation

Since the model with the extreme value for every criterion is the same in the exhautive search, we fit the specified model:
```{r}
model.allreg = lm(MonthlyIncome ~  1 + JobLevel + JobRole, data = ibm)
```

***CORRECT ME.***

The equation would be 
$$\begin{array}{rcl}
\text{Monthly_Income} &=& \beta_0 + \beta_1 \times \text{BusinessTravel} + \beta_2 \times \text{EnvironmentSatisfaction} 
                    \\ && + \beta_3 \times \text{JobInvolvement} + \beta_4 \times \text{JobRole} + \beta_5 \times \text{JobSatisfaction}
                    \\ && + \beta_6 \times \text{OverTime} + \beta_7 \times \text{StockOptionLevel} + \beta_8 \times \text{TotalWorkingYears}
\end{array}$$

#### Basic features

```{r}
summary(model.allreg)
```

```{r}
model.simple = lm(MonthlyIncome ~ JobLevel, data = ibm)
summary(model.simple)
```


#### Multicollinearity

We first check for multicollinearity:

```{r}
faraway::vif(model.allreg)
```

#### Linear assumptions
```{r}
plot(model.allreg, which = 1)
boxcox(model.allreg, lambda = seq(0.5, 1, by= 0.1))
```


```{r}
model.allreg.transform = lm(MonthlyIncome^0.7 ~  JobLevel + JobRole, data = ibm)
summary(model.allreg.transform)

plot(model.allreg.transform, which = 1)
qqline(model.allreg.transform$residuals, col="red")
boxcox(model.allreg.transform)
```

```{r}

plot(model.simple, which = 1)
boxcox(model.simple, lambda = seq(0.2, .6, by= 0.1))


#simple.transform = ibm
#simple.transform$MonthlyIncome = simple.transform$MonthlyIncome^0.5
model.simple.transform = lm(sqrt(MonthlyIncome) ~  JobLevel, data = ibm)


plot(model.simple.transform, which = 1)
{qqnorm(model.simple.transform$residuals, col="red")
qqline(model.simple.transform$residuals, col="red")}
boxcox(model.simple.transform)

```
```{r}
summary(model.allreg.transform)
summary(model.simple.transform)
```



$$ E(\varepsilon) = 0, \quad \sigma^2 \text{ constant}$$

#### Stepwise
```{r}
regnull = lm(MonthlyIncome ~  1, data = ibm)
regfull = lm(MonthlyIncome ~ ., data = ibm)

#step(regfull, scope=list(lower=regnull, upper=regfull), direction="backward")
#step(regfull, scope=list(lower=regnull, upper=regfull), direction="forward")
step(regfull, scope=list(lower=regnull, upper=regfull), direction="both", trace = 0)
```
```{r}
model.best = lm(MonthlyIncome ~ Gender + JobInvolvement + JobLevel + JobRole + 
    NumCompaniesWorked + StockOptionLevel + TotalWorkingYears + 
    YearsInCurrentRole, data = ibm)

plot(model.best, which = 1)
boxcox(model.best, lambda = seq(0.3, 1.2, 0.1))

#best.transform  = ibm
#best.transform$MonthlyIncome = best.transform$MonthlyIncome^0.7
model.best.transform = lm(MonthlyIncome^0.7 ~ Gender + JobInvolvement + JobLevel + JobRole + 
    NumCompaniesWorked + StockOptionLevel + TotalWorkingYears + 
    YearsInCurrentRole, data = ibm )

plot(model.best.transform , which = 1)
boxcox(model.best.transform, lambda = seq(0.3, 1.2, 0.1))
```
#### Transformation of non factor predictors for all 3 models (allreg, simple, best)
```{r}
transformDoubles = function(df, func) {
  dbls = sapply(df, is.double)
  copy = df
  copy[,dbls] = func(df[,dbls])
  copy
}
```


```{r}
allregMonthlyIncome = ibm$MonthlyIncome^.7
allreg.transform.predictors = ibm[-c(17)]

## We tried 3 types of transformation to all of non factor predictors (ln, )
allreg.transform.factor = allreg.transform.predictors[,sapply(allreg.transform.predictors, is.factor)]
allreg.transform.double = allreg.transform.predictors[,sapply(allreg.transform.predictors, is.double)]
#allreg.transform.double = lapply(allreg.transform.double, function(x) 1/x)
#allreg.transform.double = exp(allreg.transform.double + 0.01)
allreg.transform.double = log(allreg.transform.double + 0.01)
allreg.transform.predictors = cbind(allreg.transform.factor, allreg.transform.double)
allreg.transform.predictors$MonthlyIncome = allregMonthlyIncome
```
```{r}
# check function
res = transformDoubles(allreg.transform.predictors, function(x) { log(x + 0.01) })
res$MonthlyIncome = ibm$MonthlyIncome^.7 # retain original
#dbls = sapply(allreg.transform.predictors, is.double)
#res[,names(dbls[dbls])] == allreg.transform.predictors[,names(dbls[dbls])]
#res[,names(allreg.transform.predictors)] == allreg.transform.predictors[,names(allreg.transform.predictors)]
```


```{r}
model.allreg.transform.predictors = lm(MonthlyIncome ~  JobLevel + JobRole, data = allreg.transform.predictors)
plot(model.allreg.transform.predictors , which = 1)
boxcox(model.allreg.transform.predictors, lambda = seq(0.3, 1.2, 0.1))
```

```{r}
simpleMonthlyIncome = ibm$MonthlyIncome^.5
simple.transform.predictors = ibm[-c(17)]

## We tried 3 types of transformation to all of non factor predictors (ln, )
simple.transform.factor = simple.transform.predictors[,sapply(simple.transform.predictors, is.factor)]
simple.transform.double = simple.transform.predictors[,sapply(simple.transform.predictors, is.double)]
#simple.transform.double = lapply(simple.transform.double, function(x) 1/x)
#simple.transform.double = exp(simple.transform.double + 0.01)
simple.transform.double = log(simple.transform.double + 0.01)
simple.transform.predictors = cbind(simple.transform.factor, simple.transform.double)
simple.transform.predictors$MonthlyIncome = simpleMonthlyIncome

model.simple.transform.predictors = lm(MonthlyIncome ~  JobLevel, data = simple.transform.predictors)
plot(model.simple.transform.predictors , which = 1)
boxcox(model.simple.transform.predictors, lambda = seq(0.3, 1.2, 0.1))
```
```{r}
bestMontlyIncome = ibm$MonthlyIncome^.7
best.transform.predictors = ibm[-c(17)]

## We tried 3 types of transformation to all of non factor predictors 
best.transform.factor = best.transform.predictors[,sapply(best.transform.predictors, is.factor)]
best.transform.double = best.transform.predictors[,sapply(best.transform.predictors, is.double)]
#best.transform.double = lapply(best.transform.double, function(x) 1/x)
#best.transform.double = exp(best.transform.double + 0.01)
best.transform.double = log(best.transform.double + 0.01)
best.transform.predictors = cbind(best.transform.factor, best.transform.double)
best.transform.predictors$MonthlyIncome = bestMontlyIncome

model.best.transform.predictors = lm(MonthlyIncome ~ Gender + JobInvolvement + JobLevel + 
    JobRole + NumCompaniesWorked + StockOptionLevel + TotalWorkingYears + 
    YearsInCurrentRole, data = best.transform.predictors)
plot(model.best.transform.predictors , which = 1)
boxcox(model.best.transform.predictors, lambda = seq(0.3, 1.2, 0.1))

# Transform Y since boxcox does not contain 1
best.transform.predictors$MonthlyIncome = best.transform.predictors$MonthlyIncome^.8
model.best.transform.predictors = lm(MonthlyIncome ~ Gender + JobInvolvement + JobLevel + 
    JobRole + NumCompaniesWorked + StockOptionLevel + TotalWorkingYears + 
    YearsInCurrentRole, data = best.transform.predictors)
plot(model.best.transform.predictors , which = 1)
boxcox(model.best.transform.predictors, lambda = seq(0.3, 1.2, 0.1))
summary(model.best.transform.predictors)
```
#### Outliers Analysis
```{r}
##residuals
res = model.best.transform.predictors$residuals 
##studentized residuals
student.res = rstandard(model.best.transform.predictors) 
##externally studentized residuals
ext.student.res = rstudent(model.best.transform.predictors) 

par(mfrow = c(1,3))
plot(model.best.transform.predictors$fitted.values,res,main="Residuals")
plot(model.best.transform.predictors$fitted.values,student.res,main="Studentized Residuals")
plot(model.best.transform.predictors$fitted.values,ext.student.res,main="Externally  Studentized Residuals")

n = length(best.transform.predictors$MonthlyIncome)
p = length(coef(model.best.transform.predictors))

##critical value using Bonferroni procedure
ext.student.res[abs(ext.student.res)>qt(1-0.05/(2*n), n-p-1)]

```
```{r}
# Leverage point
lev<-lm.influence(model.best.transform.predictors)$hat 
length(lev[lev>2*p/n])

```

```{r}
ibm[which(lev>2*p/n),]
```
```{r}
plot(ibm$PerformanceRating)
```


```{r}
DFFITS<-dffits(model.best.transform.predictors)
DFFITS[abs(DFFITS)>2*sqrt(p/n)]
```
```{r}
DFBETAS<-dfbetas(model.best.transform.predictors)
length(DFBETAS[abs(DFBETAS)>2/sqrt(n)])
#length(DFBETAS)
nrow(ibm)
length(model.best.transform.predictors$residuals)
```
```{r}
COOKS<-cooks.distance(model.best.transform.predictors)
COOKS[COOKS>qf(0.5,p,n-p)]
```

#### Model Diagnostics

We now check for predictive performance.

```{r}
length(model.best.transform.predictors$coefficients)
length(ibm[,1])

```
```{r}
model.simple.rstudent     = rstudent(model.simple) 
n = nrow(ibm)
p = length(coef(model.simple))
plot(model.simple.rstudent,main="Externally Studentized Residuals", ylim=c(-4,4))
abline(h=qt(1-0.05/(2*n), n-p-1), col="red")
abline(h=-qt(1-0.05/(2*n), n-p-1), col="red")
model.simple.rstudent[abs(model.simple.rstudent)>qt(1-0.05/(2*n), n-p-1)]

```

```{r}
lev = lm.influence(model.simple)$hat 
influence = lev[lev>2*p/n]
{
  plot(lev, main="Leverages", ylim=c(-0.8,0.8))
  abline(h=2*p/n, col="red")
  identify(lev)
}

ibm[which(lev>2*p/n),]
```

```{r}
ibm.nosimplelev = ibm[-which(lev>2*p/n),]
```

```{r}
model.simple.nolev = lm(MonthlyIncome ~ JobLevel, data = ibm.nosimplelev)
plot(model.simple.nolev)
```

```{r}
plot(model.simple)
```

```{r}
# Create subsets for each job level
jobLevel1 = subset(ibm, JobLevel == 1)
jobLevel2 = subset(ibm, JobLevel == 2)
jobLevel3 = subset(ibm, JobLevel == 3)
jobLevel4 = subset(ibm, JobLevel == 4)
jobLevel5 = subset(ibm, JobLevel == 5)
```

```{r}
# using exhaustive search for job level 1
allreg.jobLevel1 <- regsubsets(MonthlyIncome ~., data=jobLevel1, nbest=1, really.big = T)
```

```{r}
# showing the outcome of exhaustive search for job level 1
pp_allreg(allreg.jobLevel1)
```

```{r}
# fit regression model for best r2 model for job level 1
model.jobLevel1 = lm(MonthlyIncome ~ 1 + Attrition + JobRole + NumCompaniesWorked + PercentSalaryHike + PerformanceRating + TotalWorkingYears + TrainingTimesLastYear + YearsInCurrentRole + YearsSinceLastPromotion, data=jobLevel1)

summary(model.jobLevel1)
plot(model.jobLevel1, which=1:2)
```

```{r}
# using exhaustive search for job level 2
allreg.jobLevel2 <- regsubsets(MonthlyIncome ~., data=jobLevel2, nbest=1, really.big = T)
```

```{r}
# showing the outcome of exhaustive search for job level 2
pp_allreg(allreg.jobLevel2)
```

```{r}
# fit regression model for best r2 model for job level 2
model.jobLevel2 = lm(MonthlyIncome ~ 1 + BusinessTravel + DailyRate + Education + EducationField + JobInvolvement + JobRole + TotalWorkingYears, data=jobLevel2)

summary(model.jobLevel2)
plot(model.jobLevel2, which=1:2)
```

```{r}
# using exhaustive search for job level 3
allreg.jobLevel3 <- regsubsets(MonthlyIncome ~., data=jobLevel3, nbest=1, really.big = T)
```

```{r}
# showing the outcome of exhaustive search for job level 3
pp_allreg(allreg.jobLevel3)
```

```{r}
# fit regression model for best r2 model for job level 3
model.jobLevel3 = lm(MonthlyIncome ~ 1 + BusinessTravel + Education + JobRole + NumCompaniesWorked + RelationshipSatisfaction + StockOptionLevel + TotalWorkingYears, data=jobLevel3)

summary(model.jobLevel3)
plot(model.jobLevel3, which=1:2)
```


```{r}
# using exhaustive search for job level 4
allreg.jobLevel4 <- regsubsets(MonthlyIncome ~., data=jobLevel4, nbest=1, really.big = T)
```

```{r}
# showing the outcome of exhaustive search for job level 4
pp_allreg(allreg.jobLevel4)
```

```{r}
# fit regression model for best r2 model for job level 4
model.jobLevel4 = lm(MonthlyIncome ~ 1 + Education + EnvironmentSatisfaction + JobRole + JobSatisfaction + MaritalStatus + MonthlyRate + PerformanceRating + RelationshipSatisfaction, data=jobLevel4)

summary(model.jobLevel4)
plot(model.jobLevel4, which=1:2)
```


```{r}
# using exhaustive search for job level 5
allreg.jobLevel5 <- regsubsets(MonthlyIncome ~., data=jobLevel5, nbest=1, really.big = T)
```

```{r}
# showing the outcome of exhaustive search for job level 5
pp_allreg(allreg.jobLevel5)
```
```{r}
# fit regression model for best r2 model for job level 5
model.jobLevel5 = lm(MonthlyIncome ~ 1 + BusinessTravel + DailyRate + Education + EducationField + EducationField + PercentSalaryHike + RelationshipSatisfaction + StockOptionLevel, data=jobLevel5)

summary(model.jobLevel5)
plot(model.jobLevel5, which=1:2)
```
