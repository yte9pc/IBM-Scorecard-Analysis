---
title: "Employee Attrition"
output: pdf_document
urlcolor: blue
author: Jon Gomez (jag2j), Michael Langmayr, Nathan England, and Yihnew Eshetu
---


# About the data

We will analyze the "IBM HR Analytics Employee Attrition & Performance" dataset ((link)[https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset]).  The dataset documentation says that it was synthesized by data scientists at IBM.  Observations describe hypothetical employees.  Rows measure the employee retention status, metrics about the workplace, and details about the employee.

# Preparation

## Libraries

```{r echo=FALSE}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(leaps))     # model selection tools
suppressPackageStartupMessages(library(faraway))   # VIF function
suppressPackageStartupMessages(library(MASS))       # Box-Cox
source('./lib/fmrhs.R') # requires tidyverse
source('./lib/Pretty correlation.R')
```

## Loading the data
We load the data using a custom column mapping.  A few considerations apply:

* Several columns are originally numeric levels.  We recode these from the data description.
* A few columns are irrelevant or constant across the data.  We drop these.

```{r echo=FALSE}
# ordered factor
ordered_factor = col_factor(levels = 1:5, ordered = F)

# column mappings
types = cols(
  # continuous variables
  Age = col_double(),              DailyRate = col_double(),  
  DistanceFromHome = col_double(), HourlyRate = col_double(),       
  #JobLevel = col_double(),
  JobLevel = col_factor(),
  # ordered factors we later recode
  #   e.g., JobSatisfaction ranges from 1 - 4 (= 'Low' to 'Very High')
  Education                = col_factor(levels = 1:5, ordered = F),
  EnvironmentSatisfaction  = col_factor(1:4, ordered = F),
  JobInvolvement           = col_factor(1:4, ordered = F),
  JobSatisfaction          = col_factor(levels = 1:4, ordered = F),
  PerformanceRating        = col_factor(levels = 1:4, ordered = F),
  RelationshipSatisfaction = col_factor(levels = 1:4, ordered = F),
  WorkLifeBalance          = col_factor(levels = 1:4, ordered = F),
  
  # other factors
  BusinessTravel =  col_factor(levels = c('Non-Travel', 'Travel_Rarely', 'Travel_Frequently'), 
                               ordered = F),
  Gender = col_factor(),            JobRole = col_factor(),
  MaritalStatus = col_factor(),     EducationField = col_factor(),
  
  # things we have decided to treat as factors
  Department = col_factor(),        # we presume we could map to a taxonomy
  StockOptionLevel = col_factor(),  # unclear scale
  
  # true/false
  Attrition = col_factor(),
  OverTime = col_factor(),
  
  # drop
  EmployeeCount = col_skip(),  # always 1
  StandardHours = col_skip(),  # always 80
  EmployeeNumber = col_skip(), # 1, 2, ...
  Over18 = col_skip(),         # always true
  
  # continous values
  MonthlyIncome = col_double(),       MonthlyRate = col_double(),              
  NumCompaniesWorked = col_double(),  PercentSalaryHike = col_double(),
  TotalWorkingYears = col_double(),   TrainingTimesLastYear = col_double(),
  WorkLifeBalance = col_double(),     YearsAtCompany = col_double(),           
  YearsInCurrentRole = col_double(),  YearsSinceLastPromotion = col_double(),
  YearsWithCurrManager = col_double()
)

# read in using mapping
ibm = read_csv("data/ibm.csv", col_types = types)

# rename levels
ibm$Education = recode(ibm$Education, 
                       '1' = 'Below College', '2' = 'College', '3' = 'Bachelor', '4' = 'Master', '5' = 'Doctor')
ibm$EnvironmentSatisfaction = recode(ibm$EnvironmentSatisfaction, 
                       '1' = 'Low', '2' = 'Medium', '3' = 'High', '4' = 'Very High')
ibm$JobInvolvement = recode(ibm$JobInvolvement, 
                       '1' = 'Low', '2' = 'Medium', '3' = 'High', '4' = 'Very High')
ibm$JobSatisfaction = recode(ibm$JobSatisfaction, 
                       '1' = 'Low', '2' = 'Medium', '3' = 'High', '4' = 'Very High')
ibm$PerformanceRating = recode(ibm$PerformanceRating, 
                       '1' = 'Low', '2' = 'Good', '3' = 'Excellent', '4' = 'Outstanding')
ibm$RelationshipSatisfaction = recode(ibm$RelationshipSatisfaction, 
                       '1' = 'Low', '2' = 'Medium', '3' = 'High', '4' = 'Very High')
ibm$WorkLifeBalance = recode(ibm$WorkLifeBalance, 
                       '1' = 'Bad', '2' = 'Good', '3' = 'Better', '4' = 'Best')

# set contrasts for yes/no levels
ibm$Attrition = relevel(ibm$Attrition, ref = "No")
ibm$OverTime = relevel(ibm$OverTime, ref = "No")
ibm$JobLevel = relevel(ibm$JobLevel, ref = 1)

# produce a de-factored version (for building intuition)
ibm.defactored = mutate_if(ibm, is.factor, ~ as.numeric(.x))
```

## General observations

There are 1,470 rows and 31 columns.  The features include both factors and quantitative variables.  We show a sample observation below.

```{r echo=FALSE}
#t(head(ibm, n = 1)) # a sample observation
```


      Age                      "41"                   MonthlyIncome            "5993"           
      Attrition                "Yes"                  MonthlyRate              "19479"          
      BusinessTravel           "Travel_Rarely"        NumCompaniesWorked       "8"              
      DailyRate                "1102"                 OverTime                 "Yes"            
      Department               "Sales"                PercentSalaryHike        "11"             
      DistanceFromHome         "1"                    PerformanceRating        "Excellent"      
      Education                "College"              RelationshipSatisfaction "Low"            
      EducationField           "Life Sciences"        StockOptionLevel         "0"              
      EnvironmentSatisfaction  "Medium"               TotalWorkingYears        "8"              
      Gender                   "Female"               TrainingTimesLastYear    "0"               
      HourlyRate               "94"                   WorkLifeBalance          "Bad"            
      JobInvolvement           "High"                 YearsAtCompany           "6"              
      JobLevel                 "2"                    YearsInCurrentRole       "4"              
      JobRole                  "Sales Executive"      YearsSinceLastPromotion  "0"              
      JobSatisfaction          "Very High"            YearsWithCurrManager     "5"    
      MaritalStatus            "Single"         


## Correlation

We look at columns for which the correlation is greater than 70 in the defactored data:
```{r echo=FALSE}
p = round(cor(ibm.defactored) * 100)
for(i in 1:nrow(p)) {
  for(j in i:ncol(p)) {
    if(i == j) next
    val = p[i,j]
    if(abs(val) > 70) {
      r = row.names(p)[i]
      c = colnames(p)[j]
      str = paste("cor(", r, ", ", c, ") = ", val, "\n")
      cat(str)
    }
  }
}
```


# Predicting monthly income

## Selecting initial models

We use the leaps library to evaluate possible variables to include.  Given the fairly small data set, we perform an exhaustive search using `regsubsets`.  This provides the following results:

```{r include = FALSE}
# load existing computation
load(file = "allreg.saved.R")
# this took about six minutes on one computer tested
#system.time({
#  allreg = regsubsets(MonthlyIncome ~ ., ibm, nbest = 1, really.big = T)
#})
#save(list=c("allreg"), file = "allreg.saved.R")
```
```{r echo=FALSE}
pp_allreg(allreg)
```

Since the model with the extreme value for every criterion is the same in the exhautive search, we fit the specified model.  We also fit the simplified model with only JobLevel, since it has such a high correlation to monthly income (as seen in the "Preparation" section).
```{r echo=FALSE}
model.allreg = lm(MonthlyIncome ~  1 + JobLevel + JobRole, data = ibm)
model.simple = lm(MonthlyIncome ~ JobLevel, data = ibm)

summary(model.allreg)
summary(model.simple)
```

## Adequacy and Box-Cox
When plotting the residuals, we find that both models have non-constant variance.  Further, they exhibit strong clumping due to the use of factorized levels as predictors:

```{r echo=FALSE}
{par(mfrow=c(1,2))
plot(model.simple, which = 1); title("(simple model)", line=-1, adj=0.95, cex.main=0.8)
plot(model.allreg, which = 1); title("(allreg model)", line=-1, adj=0.95, cex.main=0.8)}
```

We first try to fix these problems with Box-Cox.  We pick transformations using $\lambda = 0.5$ (sqrt) and $\lambda = 0.7$ for the simple and allreg models respectively.

```{r echo=FALSE}
{ par(mfrow=c(1,2))
  boxcox(model.simple, lambda = seq(0.2, .6, by= 0.1)); title("simple model")
  boxcox(model.allreg, lambda = seq(0.5, 1, by= 0.1)); title("allreg model")  }
```

A second round of Box-Cox results in confidence intervals containing 1 for both models.  We also see that the p-value for two indicator variables have high p-values in the allreg model, but the other indicator variables remain viable.
```{r echo=FALSE}
# boxcox transform simple model
model.simple.transform = lm(log(MonthlyIncome) ~  JobLevel, data = ibm)
s = summary(model.simple.transform)$coefficients
cat("\nSimple model ===> predictors with p-value > 0.5")
res = s[which(s[,4] > 0.05),]
if(nrow(res) == 0) cat("\tNone!")

# box cox transform allreg
model.allreg.transform = lm(MonthlyIncome^0.7 ~  JobLevel + JobRole, data = ibm)
s = summary(model.allreg.transform)$coefficients
cat("Allreg model ===> predictors with p-value > 0.5")
s[which(s[,4] > 0.05),]
```
```{r include=FALSE}
# (use this to print out) the box cox results after transformation
{ par(mfrow=c(1,2))
  boxcox(model.simple.transform, lambda = seq(2.2, 6, by= 0.1)); title("simple model")
  boxcox(model.allreg.transform, lambda = seq(0.8, 1.2, by= 0.1)); title("allreg model")  }
```

More problematically, the constant variance assumption still appears not to be met:

```{r echo=FALSE}
{par(mfrow=c(1,2))
plot(model.simple.transform, which = 1); title("(simple model)", line=-1, adj=0.95, cex.main=0.8)
plot(model.allreg.transform, which = 1); title("(allreg model)", line=-1, adj=0.95, cex.main=0.8)}
```

## Leverage points
We now turn to leverage points, and we come to an important realization.  When we remove them from the simple model, we drop entire job levels:
```{r echo=FALSE}
model = model.simple
ext.student.res = rstudent(model) 
n = length(model$residuals)
p = length(coef(model))
# Leverage points
lev = lm.influence(model)$hat
levpts = lev[lev>2*p/n]
# cook's distance
COOKS<-cooks.distance(model)
cookpts = COOKS[COOKS>qf(0.5,p,n-p)]
# counts
c(
  "# of leverage points" = length(levpts),
  "# of Cook's distance relevant points" =  length(cookpts)
)
```

```{r}
model = model.simple
{par(mfrow=c(1,2))
plot(model$residuals ~ ibm$JobLevel, main = "Leverage points included", xlab="JobLevel", ylab="Residuals")
plot(model$residuals[-which(lev>2*p/n)] ~ ibm$JobLevel[-which(lev>2*p/n)], main = "High leverage points removed", xlab="JobLevel", ylab="Residuals")}
```

At this point, we decided that it made sense to consider individual models for each individual job level since they have such different characteristics.  In retrospect, this is another way to interpret the box-and-whisker plots.

## Individual models

```{r}
# Create subsets for each job level
jobLevel1 = subset(ibm, JobLevel == 1)
jobLevel2 = subset(ibm, JobLevel == 2)
jobLevel3 = subset(ibm, JobLevel == 3)
jobLevel4 = subset(ibm, JobLevel == 4)
jobLevel5 = subset(ibm, JobLevel == 5)
```

```{r}
# using exhaustive search for each job level
# reload existing results
load(file="models by job level - images and object dumps/joblevel_data.R", verbose = TRUE)
# run again
#allreg.jobLevel1 = regsubsets(MonthlyIncome ~., data=jobLevel1, nbest=1, really.big = T)
#allreg.jobLevel2 = regsubsets(MonthlyIncome ~., data=jobLevel2, nbest=1, really.big = T)
#allreg.jobLevel3 = regsubsets(MonthlyIncome ~., data=jobLevel3, nbest=1, really.big = T)
#allreg.jobLevel4 = regsubsets(MonthlyIncome ~., data=jobLevel4, nbest=1, really.big = T)
#allreg.jobLevel5 = regsubsets(MonthlyIncome ~., data=jobLevel5, nbest=1, really.big = T)
```

```{r}
# fit regression model for best r2 model for job level 1
cat("=== joblevel1\n")
pp_allreg(allreg.jobLevel1)
cat("=== joblevel2\n")
pp_allreg(allreg.jobLevel2)
cat("=== joblevel3\n")
pp_allreg(allreg.jobLevel3)
cat("=== joblevel4\n")
pp_allreg(allreg.jobLevel4)
cat("=== joblevel5\n")
pp_allreg(allreg.jobLevel5)
```

We fit two of the better models:
```{r}
model.joblevel3 = lm(MonthlyIncome ~ 1 + BusinessTravel + Education + JobRole + NumCompaniesWorked + RelationshipSatisfaction  + StockOptionLevel + TotalWorkingYears, data = jobLevel3)
model.joblevel4 = lm(MonthlyIncome ~ 1 + Education  + EnvironmentSatisfaction + JobRole + MaritalStatus + MonthlyRate + PerformanceRating + RelationshipSatisfaction, data = jobLevel4)
```




## Model adequacy, continued

At this point, we want to assess final adquacy.

QQ plot.
```{r}
{
  par(mfrow=c(2,2))
  qqnorm(model.simple$residuals); title("simple", adj=0.1, line=-1)
  qqline(model.simple$residuals)
  
  qqnorm(model.allreg.transform$residuals); title("allreg.transformed", adj=0.1, line=-1)
  qqline(model.allreg.transform$residuals)
  
  qqnorm(model.joblevel3$residuals); title("joblevel3", adj=0.1, line=-1)
  qqline(model.joblevel3$residuals)
  
  qqnorm(model.joblevel4$residuals); title("joblevel4", adj=0.1, line=-1)
  qqline(model.joblevel4$residuals)
}
```

Residual plots for the job level models.  These look good.
```{r}
{
  par(mfrow=c(1,2))
  with(model.joblevel3, plot(residuals ~ fitted.values, main="joblevel3"))
  abline(h = 0, col="Red")
  with(model.joblevel4, plot(residuals ~ fitted.values, main="joblevel4"))
  abline(h = 0, col="Red")
}
```

ACFs look fine.
```{r}
{
  par(mfrow=c(2,2))
  acf(model.simple$residuals)
  acf(model.allreg.transform$residuals)
  acf(model.joblevel3$residuals)
  acf(model.joblevel4$residuals)
}
```


## Predictive performance

We look at one or two models briefly.




## Final recommendations

We consider the following models:

| name | formula |
| --- | ---- |
| model.simple | lm(MonthlyIncome ~  JobLevel, data = ibm) |
| model.allreg.transformed | lm(MonthlyIncome^0.7 ~  JobLevel + JobRole, data = ibm) |
| model.joblevel3 | lm(MonthlyIncome ~ BusinessTravel + Education + JobRole + NumCompaniesWorked + RelationshipSatisfaction  + StockOptionLevel + TotalWorkingYears) |
| model.joblevel4 | lm(MonthlyIncome ~  Education  + EnvironmentSatisfaction + JobRole + MaritalStatus + MonthlyRate + PerformanceRating + RelationshipSatisfaction) |

For the simplest model, we recommend model.simple (not the transformed one), which fits to each job level an average value.  While quick and highly predictive, with an R squared of around 0.93, the model does not meet the constant variance assumption needed for effective use of hypothesis tests.  Furthermore, it is also so simplistic that it groups employees into a few preset groups.  For slightly the best overall fit for the general employee, we recommend using the model.allreg.transformed variant.

However, the different levels of jobs appear to be rather distinctive based on the box-and-whiskers graph of the residuals against the job levels in these models.  For this reason, we investigated looking at models that specialize in particular job levels.  Two of these, model.joblevel3 and model.joblevel4 had decent values of R squared, respectively 0.70 and 0.82.  These also deal with a broader range of predictors and distinguish a larger variety of employee.  The residual plots and qq plots look much better for these models than for the general models.

These models offer some approaches to predicting employee monthly income.  The best method will ultimately depend on the specific goals informing the analysis.


## Extra: Stepwise selection

In this section, we consider some alternative models generated from stepwise selection models.  While stepwise selection may report a model with inferior values for our criteria compared to the exhaustive search, the reported model may still provide a better model in terms of linearity assumptions.  For this reason, we do consider a stepwise selection.

### Initial fit and transform
We use the `step` function with `both` directions.
```{r echo=FALSE}
regnull = lm(MonthlyIncome ~  1, data = ibm)
regfull = lm(MonthlyIncome ~ ., data = ibm)
model.step = step(regfull, scope=list(lower=regnull, upper=regfull), direction="both", trace = 0)
formula.model.step = model.step$call$formula
model.step$call
```

The diagnostic plots are not that much better.
```{r}
{par(mfrow=c(1,3))
plot(model.step, which = 1); title("(model.step)", line=-1, adj=0.95, cex.main=0.8)
plot(model.step$residuals ~ ibm$JobLevel)
boxcox(model.step, lambda = seq(0.5,1, by=0.01))}
```

With the Box-Cox transform, we get the following:

```{r echo=FALSE}
model.step.transformed = lm(formula = MonthlyIncome^0.7 ~ Gender + JobInvolvement + JobLevel + 
    JobRole + NumCompaniesWorked + StockOptionLevel + TotalWorkingYears + 
    YearsInCurrentRole, data = ibm)
{par(mfrow=c(1,2))
plot(model.step.transformed, which = 1); title("(model.step.transformed)", line=-1, adj=0.95, cex.main=0.8)
plot(model.step.transformed$residuals ~ ibm$JobLevel, main = "Transformed step model")}
```

### Additional remediation
We continue our efforts and perform some transforms to the (quantitative) predictors.  These are motivated by a desire to transform the shape of the residuals.  In particular, we use the log and multiplicative inverse transforms on the predictors.  Unfortunately, this does not really help.

```{r include=FALSE}
# helper function
transformDoubles = function(df, func) {
  dbls = sapply(df, is.double)
  copy = df
  copy[,dbls] = func(df[,dbls])
  copy
}
```
```{r echo=FALSE}
# log transform of predictors (but not response)
ibm.log = transformDoubles(subset(ibm, select = -MonthlyIncome), function(x) { log(x + 0.01) })
ibm.log$MonthlyIncome = ibm$MonthlyIncome^.7 #keep box cox transform

# inverse transform predictors and response
ibm.inv = transformDoubles(ibm, function(x) { 1 / x })
#   remove bad values (dangerous, and probably doesn't help our variance!)
ibm.inv = ibm.inv[-which(apply(ibm.inv == Inf,1, function(x) { any(x) })),]

# fit
model.step.transformed.log = lm(formula = formula.model.step, data = ibm.log)
model.step.transformed.inv = lm(formula = formula.model.step, data = ibm.inv)

# plot
{par(mfrow=c(1,2))
plot(model.step.transformed.log$residuals ~ ibm$JobLevel, main = "Transformed step model")
plot(model.step.transformed.inv$residuals ~ ibm.inv$JobLevel, main = "Transformed step model")}

```




